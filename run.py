from transformers import Qwen2_5_VLForConditionalGeneration, AutoProcessor
import torch
from PIL import Image
from typing import List, Dict
import os
# ==================== â­ Ø§Ù„ØªØ¹Ø¯ÙŠÙ„ Ø§Ù„Ø£ÙˆÙ„: ØªØ­Ø¯ÙŠØ¯ GPU ====================
#os.environ["CUDA_VISIBLE_DEVICES"] = "0"  # Ø§Ø³ØªØ®Ø¯Ø§Ù… GPU Ø§Ù„Ø«Ø§Ù†ÙŠØ© ÙÙ‚Ø·

# ==================== Ø¯Ø§Ù„Ø© process_vision_info (Ù…Ø­Ø¯Ø«Ø©) ====================
def process_vision_info(messages: List[dict]):
    image_inputs = []
    video_inputs = []
    for message in messages:
        if isinstance(message["content"], list):
            for item in message["content"]:
                if item["type"] == "image":
                    image = item["image"]
                    if isinstance(image, str):
                        # ÙØªØ­ Ø§Ù„ØµÙˆØ±Ø© Ù…Ø¹ ØªØ­Ø³ÙŠÙ† Ø§Ù„Ø¬ÙˆØ¯Ø©
                        image = Image.open(image).convert("RGB")
                    elif isinstance(image, Image.Image):
                        pass
                    else:
                        raise ValueError(f"Unsupported image type: {type(image)}")
                    image_inputs.append(image)
                elif item["type"] == "video":
                    video_inputs.append(item["video"])
    return image_inputs if image_inputs else None, video_inputs if video_inputs else None

# ØªØ­Ù…ÙŠÙ„ Ø§Ù„Ù†Ù…ÙˆØ°Ø¬ ÙˆØ§Ù„Ù…Ø¹Ø§Ù„Ø¬
model_name = "sherif1313/Arabic-English-handwritten-OCR-v3"

# ØªØ­Ù…ÙŠÙ„ Ø§Ù„Ù†Ù…ÙˆØ°Ø¬ Ù…Ø¹ Ø¥Ø¹Ø¯Ø§Ø¯Ø§Øª Ù…Ø­Ø³Ù†Ø©
model = Qwen2_5_VLForConditionalGeneration.from_pretrained(
    model_name,
    dtype=torch.bfloat16,
    device_map="auto",
    trust_remote_code=True
)

# Ø¥Ø¹Ø¯Ø§Ø¯ Ø§Ù„Ù…Ø¹Ø§Ù„Ø¬ Ù…Ø¹ Ø¯Ù‚Ø© Ø£Ø¹Ù„Ù‰ Ù„ØªØ­Ø³ÙŠÙ† Ø¯Ù‚Ø© OCR
processor = AutoProcessor.from_pretrained(
    model_name,
    trust_remote_code=True
)

def extract_text_from_image(image_path):
    try:
        # âœ… Ø§Ø³ØªØ®Ø¯Ø§Ù… prompt Ø£ÙƒØ«Ø± ÙˆØ¶ÙˆØ­Ù‹Ø§ ÙˆØ·Ù„Ø¨Ø§ Ù„Ù„Ù†Øµ Ø§Ù„ÙƒØ§Ù…Ù„
        messages = [
            {
                "role": "user",
                "content": [
                    {"type": "image", "image": image_path},
                    {"type": "text", "text": "Ø§Ø±Ø¬Ùˆ Ø§Ø³ØªØ®Ø±Ø§Ø¬ Ø§Ù„Ù†Øµ Ø§Ù„Ø¹Ø±Ø¨ÙŠ ÙƒØ§Ù…Ù„Ø§Ù‹ Ù…Ù† Ù‡Ø°Ù‡ Ø§Ù„ØµÙˆØ±Ø© Ù…Ù† Ø§Ù„Ø¨Ø¯Ø§ÙŠØ© Ø§Ù„Ù‰ Ø§Ù„Ù†Ù‡Ø§ÙŠØ© Ø¨Ø¯ÙˆÙ† Ø§ÙŠ Ø§Ø®ØªØµØ§Ø± ÙˆØ¯ÙˆÙ† Ø°ÙŠØ§Ø¯Ø© Ø§Ùˆ Ø­Ø°Ù. Ø§Ù‚Ø±Ø£ ÙƒÙ„ Ø§Ù„Ù…Ø­ØªÙˆÙ‰ Ø§Ù„Ù†ØµÙŠ Ø§Ù„Ù…ÙˆØ¬ÙˆØ¯ ÙÙŠ Ø§Ù„ØµÙˆØ±Ø©:"},
                ],
            }
        ]

        # ØªØ­Ø¶ÙŠØ± Ø§Ù„Ù†Øµ ÙˆØ§Ù„ØµÙˆØ±
        text = processor.apply_chat_template(
            messages, tokenize=False, add_generation_prompt=True
        )
        image_inputs, video_inputs = process_vision_info(messages)
        
        # Ù…Ø¹Ø§Ù„Ø¬Ø© Ø§Ù„Ù…Ø¯Ø®Ù„Ø§Øª Ù…Ø¹ Ø¥Ø¹Ø¯Ø§Ø¯Ø§Øª Ù…Ø­Ø³Ù†Ø©
        inputs = processor(
            text=[text],
            images=image_inputs,
            padding=True,
            return_tensors="pt",
        ).to(model.device)

        # âœ… Ø¥Ø¹Ø¯Ø§Ø¯Ø§Øª ØªÙˆÙ„ÙŠØ¯ Ù…Ø­Ø³Ù†Ø© Ù„Ù„Ù†ØµÙˆØµ Ø§Ù„Ø·ÙˆÙŠÙ„Ø©
        generated_ids = model.generate(
            **inputs,
            max_new_tokens=1024,  # Ø²ÙŠØ§Ø¯Ø© ÙƒØ¨ÙŠØ±Ø© Ù„Ø§Ø³ØªÙŠØ¹Ø§Ø¨ Ø§Ù„Ù†ØµÙˆØµ Ø§Ù„Ø·ÙˆÙŠÙ„Ø©
            min_new_tokens=50,   # Ø§Ù„Ø­Ø¯ Ø§Ù„Ø£Ø¯Ù†Ù‰ Ù„Ø¶Ù…Ø§Ù† Ø¹Ø¯Ù… Ø§Ù„Ù‚Øµ Ø§Ù„Ù…Ø¨ÙƒØ±
            do_sample=False,      # Ù„Ù„Ø­ØµÙˆÙ„ Ø¹Ù„Ù‰ Ù†ØªØ§Ø¦Ø¬ Ù…ØªØ³Ù‚Ø©
            temperature=0.01,      # ØªÙˆØ§Ø²Ù† Ø¨ÙŠÙ† Ø§Ù„Ø¥Ø¨Ø¯Ø§Ø¹ ÙˆØ§Ù„Ø§Ø³ØªÙ‚Ø±Ø§Ø±
            top_p=0.1,           # Ù„Ù„ØªÙ†ÙˆØ¹ Ø§Ù„Ù…Ø¹ØªØ¯Ù„
            repetition_penalty=1.1,  # Ù…Ù†Ø¹ Ø§Ù„ØªÙƒØ±Ø§Ø±
            pad_token_id=processor.tokenizer.eos_token_id,
            eos_token_id=processor.tokenizer.eos_token_id,
            num_return_sequences=1
        )

        # Ø§Ø³ØªØ®Ø±Ø§Ø¬ Ø§Ù„Ù†Øµ Ø§Ù„Ù…ÙÙˆÙ„ÙÙ‘Ø¯ ÙÙ‚Ø· (Ø¨Ø¯ÙˆÙ† Ù…Ø·Ø§Ù„Ø¨Ø© Ø§Ù„Ù…Ø³ØªØ®Ø¯Ù…)
        input_len = inputs.input_ids.shape[1]
        output_text = processor.batch_decode(
            generated_ids[:, input_len:],
            skip_special_tokens=True,
            clean_up_tokenization_spaces=True  # ØªØ­Ø³ÙŠÙ† Ø§Ù„Ù…Ø³Ø§ÙØ§Øª
        )[0]

        return output_text.strip()

    except Exception as e:
        return f"Ø­Ø¯Ø« Ø®Ø·Ø£ Ø£Ø«Ù†Ø§Ø¡ Ù…Ø¹Ø§Ù„Ø¬Ø© Ø§Ù„ØµÙˆØ±Ø©: {str(e)}"

# Ø¯Ø§Ù„Ø© Ù…Ø³Ø§Ø¹Ø¯Ø© Ù„ØªØ­Ø³ÙŠÙ† Ø¬ÙˆØ¯Ø© Ø§Ù„ØµÙˆØ±
def enhance_image_quality(image_path):
    """ØªØ­Ø³ÙŠÙ† Ø¬ÙˆØ¯Ø© Ø§Ù„ØµÙˆØ±Ø© Ù„ØªØ­Ø³ÙŠÙ† Ø¯Ù‚Ø© OCR"""
    try:
        img = Image.open(image_path)
        # Ø²ÙŠØ§Ø¯Ø© Ø§Ù„Ø¯Ù‚Ø© Ø¥Ø°Ø§ ÙƒØ§Ù†Øª Ø§Ù„ØµÙˆØ±Ø© ØµØºÙŠØ±Ø©
        if max(img.size) < 800:
            new_size = (img.size[0] * 2, img.size[1] * 2)
            img = img.resize(new_size, Image.Resampling.LANCZOS)
        return img
    except:
        return Image.open(image_path)

# Ù…Ø«Ø§Ù„ Ù„Ù„Ø§Ø³ØªØ®Ø¯Ø§Ù…
if __name__ == "__main__":
    TEST_IMAGES_DIR = "/home/sheriff/Desktop/88/untitled folder"
    IMAGE_EXTENSIONS = ['.png', '.jpg', '.jpeg', '.tif', '.tiff']

    image_files = [
        os.path.join(TEST_IMAGES_DIR, f)
        for f in os.listdir(TEST_IMAGES_DIR)
        if any(f.lower().endswith(ext) for ext in IMAGE_EXTENSIONS)
    ]

    if not image_files:
        print("âŒ Ù„Ø§ ØªÙˆØ¬Ø¯ ØµÙˆØ± ÙÙŠ Ø§Ù„Ù…Ø¬Ù„Ø¯.")
        exit()

    print(f"ğŸ” Ø§Ù„Ø¹Ø«ÙˆØ± Ø¹Ù„Ù‰ {len(image_files)} ØµÙˆØ±Ø© Ù„Ù„Ù…Ø¹Ø§Ù„Ø¬Ø©")
    
    for img_path in sorted(image_files):
        print(f"\n{'='*50}")
        print(f"ğŸ–¼ï¸ Ù…Ø¹Ø§Ù„Ø¬Ø©: {os.path.basename(img_path)}")
        print(f"{'='*50}")
        
        try:
            # âœ… Ø§Ø³ØªØ®Ø¯Ù… Ø§Ù„Ø¯Ø§Ù„Ø© Ø§Ù„Ù…Ø­Ø³Ù†Ø©
            extracted_text = extract_text_from_image(img_path)
            
            print("ğŸ“ Ø§Ù„Ù†Øµ Ø§Ù„Ù…Ø³ØªØ®Ø±Ø¬:")
            print("-" * 40)
            print(extracted_text)
            print("-" * 40)
            
            # âœ… Ø­Ø³Ø§Ø¨ Ø·ÙˆÙ„ Ø§Ù„Ù†Øµ Ù„Ù„Ù…Ù‚Ø§Ø±Ù†Ø©
            text_length = len(extracted_text)
            print(f"ğŸ“Š Ø·ÙˆÙ„ Ø§Ù„Ù†Øµ: {text_length} Ø­Ø±Ù")
            
        except Exception as e:
            print(f"âŒ Ø®Ø·Ø£ ÙÙŠ Ù…Ø¹Ø§Ù„Ø¬Ø© {os.path.basename(img_path)}: {e}")
